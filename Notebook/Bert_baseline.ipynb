{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport zipfile\nimport datetime\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['test.csv', 'train.csv', 'data_info_val_sample_submission.csv', 'categories.json']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2342df94aae49f15ac54f04d52aa41ff8c42dbc6",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "!pip install pytorch_pretrained_bert ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting pytorch_pretrained_bert\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n\u001b[K    100% |████████████████████████████████| 122kB 4.8MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (2.21.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (4.31.1)\nRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.16.2)\nRequirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (2018.1.10)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.9.106)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (2018.11.29)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (2.6)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (3.0.4)\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (1.22)\nRequirement already satisfied: botocore<1.13.0,>=1.12.106 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (1.12.106)\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (0.2.0)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.106->boto3->pytorch_pretrained_bert) (2.6.0)\nRequirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.106->boto3->pytorch_pretrained_bert) (0.14)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.106->boto3->pytorch_pretrained_bert) (1.12.0)\nInstalling collected packages: pytorch-pretrained-bert\nSuccessfully installed pytorch-pretrained-bert-0.6.1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f72198eef7ebc474308d74cd4b4b6f638c87da6"
      },
      "cell_type": "code",
      "source": "import torch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f3f942394f6bb5294048a2d707874ab8b5912e89"
      },
      "cell_type": "code",
      "source": "# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "100%|██████████| 995526/995526 [00:00<00:00, 1832875.07B/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4fe67e6df4b096b185013e1562319794f449cb3b"
      },
      "cell_type": "code",
      "source": "# Tokenized input\ntext = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\ntokenized_text = tokenizer.tokenize(text)\ntokenized_text",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "['[CLS]',\n 'who',\n 'was',\n 'jim',\n 'hen',\n '##son',\n '?',\n '[SEP]',\n 'jim',\n 'hen',\n '##son',\n 'was',\n 'a',\n 'pu',\n '##ppet',\n '##eer',\n '[SEP]']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c8297b01ae187644244d75a2f7f90ed61034b485"
      },
      "cell_type": "code",
      "source": "\nTRAIN_CSV_PATH = '../input/train.csv'\nTEST_CSV_PATH = '../input/test.csv'\n\n    \ntrain = pd.read_csv(TRAIN_CSV_PATH, index_col='itemid')\ntest = pd.read_csv(TEST_CSV_PATH, index_col='itemid')\n\ntrain.rename(columns={'Category': 'label'},inplace = True)\ntest.rename(columns={'Category': 'label'},inplace = True)\ncols = ['title', \n        'label']\ntrain = train.loc[:, cols]\ntest = test.loc[:, cols]\ntrain.fillna('class_0', inplace=True)\ntest.fillna('class_0', inplace=True)\ntrain.head(3)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:1472: FutureWarning: \nPassing list-likes to .loc or [] with any missing label will raise\nKeyError in the future, you can use .reindex() as an alternative.\n\nSee the documentation here:\nhttps://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n  return self._getitem_tuple(key)\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "                                                     title  label\nitemid                                                           \n307504                nyx sex bomb pallete natural palette      0\n461203   etude house precious mineral any cushion pearl...      1\n3592295                           milani rose powder blush      2",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>itemid</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>307504</th>\n      <td>nyx sex bomb pallete natural palette</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>461203</th>\n      <td>etude house precious mineral any cushion pearl...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3592295</th>\n      <td>milani rose powder blush</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "064648cece639e025d6543e449ff0315edb58511"
      },
      "cell_type": "code",
      "source": "test.head()",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "                                                        title    label\nitemid                                                                \n370855998                flormar 7 white cream bb spf 30 40ml  class_0\n637234604   maybelline clear smooth all in one bb cream sp...  class_0\n690282890   murah innisfree eco natural green tea bb cream...  class_0\n930913462   loreal white perfect day cream spf 17 pa white...  class_0\n1039280071  hada labo cc cream ultimate anti aging spf 35 ...  class_0",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>itemid</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>370855998</th>\n      <td>flormar 7 white cream bb spf 30 40ml</td>\n      <td>class_0</td>\n    </tr>\n    <tr>\n      <th>637234604</th>\n      <td>maybelline clear smooth all in one bb cream sp...</td>\n      <td>class_0</td>\n    </tr>\n    <tr>\n      <th>690282890</th>\n      <td>murah innisfree eco natural green tea bb cream...</td>\n      <td>class_0</td>\n    </tr>\n    <tr>\n      <th>930913462</th>\n      <td>loreal white perfect day cream spf 17 pa white...</td>\n      <td>class_0</td>\n    </tr>\n    <tr>\n      <th>1039280071</th>\n      <td>hada labo cc cream ultimate anti aging spf 35 ...</td>\n      <td>class_0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eca1d06d602beafb14fdfbad59e2f3cee0a18214"
      },
      "cell_type": "code",
      "source": "train['label'] =train['label'].apply(lambda x : \"class_\" + str(x) )\n# test['title'] =test['title'].apply(lambda x : \"class_\" + str(x) )\n# train['title'] = train['title'].astype(str).str.replace('\\d+', '')\n# test['title'] = test['title'].astype(str).str.replace('\\d+', '')",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca529f5ab9d503fa0d83aebd3116718c11b08e5f"
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "                                                     title    label\nitemid                                                             \n307504                nyx sex bomb pallete natural palette  class_0\n461203   etude house precious mineral any cushion pearl...  class_1\n3592295                           milani rose powder blush  class_2\n4460167                etude house baby sweet sugar powder  class_3\n5853995       bedak revlon color stay aqua mineral make up  class_3",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>itemid</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>307504</th>\n      <td>nyx sex bomb pallete natural palette</td>\n      <td>class_0</td>\n    </tr>\n    <tr>\n      <th>461203</th>\n      <td>etude house precious mineral any cushion pearl...</td>\n      <td>class_1</td>\n    </tr>\n    <tr>\n      <th>3592295</th>\n      <td>milani rose powder blush</td>\n      <td>class_2</td>\n    </tr>\n    <tr>\n      <th>4460167</th>\n      <td>etude house baby sweet sugar powder</td>\n      <td>class_3</td>\n    </tr>\n    <tr>\n      <th>5853995</th>\n      <td>bedak revlon color stay aqua mineral make up</td>\n      <td>class_3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "15ce49241b4fee4e95f87c3a351287a1091c0db8",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from collections import Counter\n# Counter(train.title)",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d661af4c3b79249596e9c7615bf1d3481dd4054"
      },
      "cell_type": "code",
      "source": "!wget https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/master/examples/run_classifier.py",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "--2019-03-10 16:34:16--  https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/master/examples/run_classifier.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 28675 (28K) [text/plain]\nSaving to: ‘run_classifier.py’\n\nrun_classifier.py   100%[===================>]  28.00K  --.-KB/s    in 0.01s   \n\n2019-03-10 16:34:16 (2.71 MB/s) - ‘run_classifier.py’ saved [28675/28675]\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0aa84e5478612b348eb7d57ca38cb90e043ef7b"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nVALIDATION_RATIO = 0.1\n\nRANDOM_STATE = 1234\n\ntrain, val= \\\n    train_test_split(\n        train, \n        test_size=VALIDATION_RATIO, \n        random_state=RANDOM_STATE\n)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37c0f684ddb7007aa98b6cd6d09c7eccfc9ca788"
      },
      "cell_type": "code",
      "source": "VOCAB = '../input/bert-pretrained-models/multi_cased_L-12_H-768_A-12/vocab.txt'\nMODEL = '../input/bert-pretrained-models/multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001'",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b7e409325714dcf47bb21f2de7eeb13d6266f0a1"
      },
      "cell_type": "code",
      "source": "label_list = ['class_' +str(i) for i in range(58)]\n# print(label_list)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f8bec0fd994f8bb407c5283fcc3a6d1cd388bff"
      },
      "cell_type": "code",
      "source": "from run_classifier import *\n\ntrain_examples = [InputExample(guid = 'train',text_a=row.title,label= row.label) for row in train.itertuples()]\nval_examples = [InputExample(guid = 'val',text_a=row.title,label= row.label) for row in val.itertuples()]\ntest_examples = [InputExample(guid = 'test',text_a=row.title,label='class_0') for row in test.itertuples()]\n\nlen(train_examples)",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "599953"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6bd427ee00b64f7bc09ab58eea5658d7daff3f1c"
      },
      "cell_type": "code",
      "source": "# train.head()",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b035211953fe1879c61ab995a1edb7eb4ce3c85a"
      },
      "cell_type": "code",
      "source": "orginal_total = len(train_examples)\ntrain_examples = train_examples[:int(orginal_total*0.2)]",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95185a4827e8b1be1f598ca8e4f40e6bb7209bd4"
      },
      "cell_type": "code",
      "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ngradient_accumulation_steps = 1\ntrain_batch_size = 32\neval_batch_size = 128\ntrain_batch_size = train_batch_size // gradient_accumulation_steps\noutput_dir = 'output'\nbert_model = 'bert-base-multilingual-cased'\nnum_train_epochs = 3\nnum_train_optimization_steps = int(\n            len(train_examples) / train_batch_size / gradient_accumulation_steps) * num_train_epochs\ncache_dir = \"model\"\nlearning_rate = 5e-5\nwarmup_proportion = 0.1\nmax_seq_length = 64\n",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "617e739d516c472da9c626c2aa6d843ba14eadba",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased',\n              cache_dir=cache_dir,\n              num_labels = 58)\nmodel.to(device)\nif n_gpu > 1:\n    model = torch.nn.DataParallel(model)\nmodel, tokenizer",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "03/10/2019 16:34:20 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /tmp/.pytorch_pretrained_bert/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n03/10/2019 16:34:21 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz not found in cache, downloading to /tmp/tmpfmo3ol0z\n100%|██████████| 662804195/662804195 [00:10<00:00, 65178381.74B/s]\n03/10/2019 16:34:31 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmpfmo3ol0z to cache at model/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\n03/10/2019 16:34:34 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for model/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\n03/10/2019 16:34:34 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmpfmo3ol0z\n03/10/2019 16:34:34 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz from cache at model/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\n03/10/2019 16:34:34 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file model/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9 to temp dir /tmp/tmpw8xqqhru\n03/10/2019 16:34:45 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 119547\n}\n\n03/10/2019 16:34:50 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n03/10/2019 16:34:50 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "(BertForSequenceClassification(\n   (bert): BertModel(\n     (embeddings): BertEmbeddings(\n       (word_embeddings): Embedding(119547, 768)\n       (position_embeddings): Embedding(512, 768)\n       (token_type_embeddings): Embedding(2, 768)\n       (LayerNorm): BertLayerNorm()\n       (dropout): Dropout(p=0.1)\n     )\n     (encoder): BertEncoder(\n       (layer): ModuleList(\n         (0): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n         (1): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n         (2): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n         (3): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n         (4): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n         (5): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n         (6): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n         (7): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n         (8): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n         (9): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n         (10): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n         (11): BertLayer(\n           (attention): BertAttention(\n             (self): BertSelfAttention(\n               (query): Linear(in_features=768, out_features=768, bias=True)\n               (key): Linear(in_features=768, out_features=768, bias=True)\n               (value): Linear(in_features=768, out_features=768, bias=True)\n               (dropout): Dropout(p=0.1)\n             )\n             (output): BertSelfOutput(\n               (dense): Linear(in_features=768, out_features=768, bias=True)\n               (LayerNorm): BertLayerNorm()\n               (dropout): Dropout(p=0.1)\n             )\n           )\n           (intermediate): BertIntermediate(\n             (dense): Linear(in_features=768, out_features=3072, bias=True)\n           )\n           (output): BertOutput(\n             (dense): Linear(in_features=3072, out_features=768, bias=True)\n             (LayerNorm): BertLayerNorm()\n             (dropout): Dropout(p=0.1)\n           )\n         )\n       )\n     )\n     (pooler): BertPooler(\n       (dense): Linear(in_features=768, out_features=768, bias=True)\n       (activation): Tanh()\n     )\n   )\n   (dropout): Dropout(p=0.1)\n   (classifier): Linear(in_features=768, out_features=58, bias=True)\n ), <pytorch_pretrained_bert.tokenization.BertTokenizer at 0x7ff6db7af710>)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7af28f04e1d04c908d759f83454a52320ac98df"
      },
      "cell_type": "code",
      "source": "# Prepare optimizer\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\noptimizer = BertAdam(optimizer_grouped_parameters,\n                             lr=learning_rate,\n                             warmup=warmup_proportion,\n                             t_total=num_train_optimization_steps)",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3a12e891f4411bad8c4103b0b9fd2fabace494f"
      },
      "cell_type": "code",
      "source": "tokenizer",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "<pytorch_pretrained_bert.tokenization.BertTokenizer at 0x7ff6db7af710>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6f78290618ca8ddcf0e289ae83b91123d3663cd"
      },
      "cell_type": "code",
      "source": "# label_list",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "796fb323c22639d8b31e0cc845de62d32b07e35e"
      },
      "cell_type": "code",
      "source": "train_features = convert_examples_to_features(\n    train_examples, label_list, max_seq_length, tokenizer)",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "03/10/2019 16:34:53 - INFO - run_classifier -   *** Example ***\n03/10/2019 16:34:53 - INFO - run_classifier -   guid: train\n03/10/2019 16:34:53 - INFO - run_classifier -   tokens: [CLS] bahan thick s ##cu ##ba ama ##bilis off shoulder dress pes ##ta party [SEP]\n03/10/2019 16:34:53 - INFO - run_classifier -   input_ids: 101 27961 59925 187 12352 10537 28149 48411 11898 78681 67348 59411 10213 14039 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   label: class_22 (id = 22)\n03/10/2019 16:34:53 - INFO - run_classifier -   *** Example ***\n03/10/2019 16:34:53 - INFO - run_classifier -   guid: train\n03/10/2019 16:34:53 - INFO - run_classifier -   tokens: [CLS] ter ##lari ##s silver bra ##zing past ##e flux po ##wder bu ##buk pasta pat ##ri la 330 05 ##91 [SEP]\n03/10/2019 16:34:53 - INFO - run_classifier -   input_ids: 101 12718 15187 10107 23394 67603 19308 17781 10112 61548 10514 62228 11499 58074 63941 20194 10401 10109 20564 10831 74178 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   label: class_3 (id = 3)\n03/10/2019 16:34:53 - INFO - run_classifier -   *** Example ***\n03/10/2019 16:34:53 - INFO - run_classifier -   guid: train\n03/10/2019 16:34:53 - INFO - run_classifier -   tokens: [CLS] sam ##sung gala ##xy a ##7 2018 4 64 ##g ##b gara ##nsi resmi sein [SEP]\n03/10/2019 16:34:53 - INFO - run_classifier -   input_ids: 101 21083 67007 74288 46776 169 11305 10434 125 11295 10240 10457 24457 22726 22407 11479 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   label: class_32 (id = 32)\n03/10/2019 16:34:53 - INFO - run_classifier -   *** Example ***\n03/10/2019 16:34:53 - INFO - run_classifier -   guid: train\n03/10/2019 16:34:53 - INFO - run_classifier -   tokens: [CLS] opp ##o neo 7 [SEP]\n03/10/2019 16:34:53 - INFO - run_classifier -   input_ids: 101 15153 10133 50071 128 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   label: class_41 (id = 41)\n03/10/2019 16:34:53 - INFO - run_classifier -   *** Example ***\n03/10/2019 16:34:53 - INFO - run_classifier -   guid: train\n03/10/2019 16:34:53 - INFO - run_classifier -   tokens: [CLS] urban de ##cay nak ##ed skin shape ##shi ##fter light medium shift [SEP]\n03/10/2019 16:34:53 - INFO - run_classifier -   input_ids: 101 23351 10104 69218 82035 10336 40564 31260 16119 33163 15765 29843 51467 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:34:53 - INFO - run_classifier -   label: class_0 (id = 0)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab837f75f60b57cd1e5beaef375f71176f745106"
      },
      "cell_type": "code",
      "source": "# train_examples",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3789fceaa6a347050bdca6dfdf191c1354f0daaf"
      },
      "cell_type": "code",
      "source": "global_step = 0\nnb_tr_steps = 0\ntr_loss = 0\n\ntrain_features = convert_examples_to_features(\n    train_examples, label_list, max_seq_length, tokenizer)\nlogger.info(\"***** Running training *****\")\nlogger.info(\"  Num examples = %d\", len(train_examples))\nlogger.info(\"  Batch size = %d\", train_batch_size)\nlogger.info(\"  Num steps = %d\", num_train_optimization_steps)\nall_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\nall_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\nall_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\nall_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\ntrain_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n\nmodel.train()\nfor _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    total_step = len(train_data) // train_batch_size\n    ten_percent_step = total_step // 10\n    for step, batch in enumerate(train_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        input_ids, input_mask, segment_ids, label_ids = batch\n        loss = model(input_ids, segment_ids, input_mask, label_ids)\n        if n_gpu > 1:\n            loss = loss.mean() # mean() to average on multi-gpu.\n        if gradient_accumulation_steps > 1:\n            loss = loss / gradient_accumulation_steps\n\n        loss.backward()\n\n        tr_loss += loss.item()\n        nb_tr_examples += input_ids.size(0)\n        nb_tr_steps += 1\n        if (step + 1) % gradient_accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1\n            \n        if step % ten_percent_step == 0:\n            print(\"Fininshed: {:.2f}% ({}/{})\".format(step/total_step*100, step, total_step))",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "03/10/2019 16:35:14 - INFO - run_classifier -   *** Example ***\n03/10/2019 16:35:14 - INFO - run_classifier -   guid: train\n03/10/2019 16:35:14 - INFO - run_classifier -   tokens: [CLS] bahan thick s ##cu ##ba ama ##bilis off shoulder dress pes ##ta party [SEP]\n03/10/2019 16:35:14 - INFO - run_classifier -   input_ids: 101 27961 59925 187 12352 10537 28149 48411 11898 78681 67348 59411 10213 14039 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   label: class_22 (id = 22)\n03/10/2019 16:35:14 - INFO - run_classifier -   *** Example ***\n03/10/2019 16:35:14 - INFO - run_classifier -   guid: train\n03/10/2019 16:35:14 - INFO - run_classifier -   tokens: [CLS] ter ##lari ##s silver bra ##zing past ##e flux po ##wder bu ##buk pasta pat ##ri la 330 05 ##91 [SEP]\n03/10/2019 16:35:14 - INFO - run_classifier -   input_ids: 101 12718 15187 10107 23394 67603 19308 17781 10112 61548 10514 62228 11499 58074 63941 20194 10401 10109 20564 10831 74178 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   label: class_3 (id = 3)\n03/10/2019 16:35:14 - INFO - run_classifier -   *** Example ***\n03/10/2019 16:35:14 - INFO - run_classifier -   guid: train\n03/10/2019 16:35:14 - INFO - run_classifier -   tokens: [CLS] sam ##sung gala ##xy a ##7 2018 4 64 ##g ##b gara ##nsi resmi sein [SEP]\n03/10/2019 16:35:14 - INFO - run_classifier -   input_ids: 101 21083 67007 74288 46776 169 11305 10434 125 11295 10240 10457 24457 22726 22407 11479 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   label: class_32 (id = 32)\n03/10/2019 16:35:14 - INFO - run_classifier -   *** Example ***\n03/10/2019 16:35:14 - INFO - run_classifier -   guid: train\n03/10/2019 16:35:14 - INFO - run_classifier -   tokens: [CLS] opp ##o neo 7 [SEP]\n03/10/2019 16:35:14 - INFO - run_classifier -   input_ids: 101 15153 10133 50071 128 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   label: class_41 (id = 41)\n03/10/2019 16:35:14 - INFO - run_classifier -   *** Example ***\n03/10/2019 16:35:14 - INFO - run_classifier -   guid: train\n03/10/2019 16:35:14 - INFO - run_classifier -   tokens: [CLS] urban de ##cay nak ##ed skin shape ##shi ##fter light medium shift [SEP]\n03/10/2019 16:35:14 - INFO - run_classifier -   input_ids: 101 23351 10104 69218 82035 10336 40564 31260 16119 33163 15765 29843 51467 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n03/10/2019 16:35:14 - INFO - run_classifier -   label: class_0 (id = 0)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28ba6b31c6096e88a218ad070ed40f34d9a015d9"
      },
      "cell_type": "code",
      "source": " if not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Save a trained model and the associated configuration\nmodel_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\noutput_model_file = os.path.join(output_dir, WEIGHTS_NAME)\ntorch.save(model_to_save.state_dict(), output_model_file)\noutput_config_file = os.path.join(output_dir, CONFIG_NAME)\nwith open(output_config_file, 'w') as f:\n    f.write(model_to_save.config.to_json_string())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a893a90ab8110fd6d6e1bd044269c2e77cf2cc3"
      },
      "cell_type": "code",
      "source": "# Load a trained model and config that you have fine-tuned\nconfig = BertConfig(output_config_file)\nmodel = BertForSequenceClassification(config, num_labels=len(label_list))\nmodel.load_state_dict(torch.load(output_model_file))\nmodel.to(device)  # important to specific device\nif n_gpu > 1:\n    model = torch.nn.DataParallel(model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "285ac54f9072c6709b13e825a5727f6af5fb2cdd"
      },
      "cell_type": "code",
      "source": "config",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "877dc7f3f770546d93e7748398ded13002ea6c84"
      },
      "cell_type": "code",
      "source": "# val\neval_examples = val_examples\neval_features = convert_examples_to_features(\n    eval_examples, label_list, max_seq_length, tokenizer)\nlogger.info(\"***** Running evaluation *****\")\nlogger.info(\"  Num examples = %d\", len(eval_examples))\nlogger.info(\"  Batch size = %d\", eval_batch_size)\nall_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\nall_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\nall_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\nall_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\neval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n# Run prediction for full data\neval_sampler = SequentialSampler(eval_data)\neval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n\nmodel.eval()\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n\nfor input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n    input_ids = input_ids.to(device)\n    input_mask = input_mask.to(device)\n    segment_ids = segment_ids.to(device)\n    label_ids = label_ids.to(device)\n\n    with torch.no_grad():\n        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n        logits = model(input_ids, segment_ids, input_mask)\n\n    logits = logits.detach().cpu().numpy()\n    label_ids = label_ids.to('cpu').numpy()\n    tmp_eval_accuracy = accuracy(logits, label_ids)\n\n    eval_loss += tmp_eval_loss.mean().item()\n    eval_accuracy += tmp_eval_accuracy\n\n    nb_eval_examples += input_ids.size(0)\n    nb_eval_steps += 1\n\neval_loss = eval_loss / nb_eval_steps\neval_accuracy = eval_accuracy / nb_eval_examples\nloss = tr_loss/nb_tr_steps\nresult = {'eval_loss': eval_loss,\n          'eval_accuracy': eval_accuracy,\n          'global_step': global_step,\n          'loss': loss}\n\noutput_eval_file = os.path.join(output_dir, \"eval_results.txt\")\nwith open(output_eval_file, \"w\") as writer:\n    logger.info(\"***** Eval results *****\")\n    for key in sorted(result.keys()):\n        logger.info(\"  %s = %s\", key, str(result[key]))\n        writer.write(\"%s = %s\\n\" % (key, str(result[key])))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24770c1afe69ddd4ae6b6038caf4ca5dd7660116"
      },
      "cell_type": "code",
      "source": "device",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c15be0fd8cf8e5906d92c1f7dbbcf3a733c31c90"
      },
      "cell_type": "code",
      "source": "!ls output\n! cat output/eval_results.txt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be5df5882cec766558ccc8fb6be4db487edb7600"
      },
      "cell_type": "code",
      "source": "model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b9a839f46d4577137978e471b0bc299576917e10"
      },
      "cell_type": "code",
      "source": "def predict(model, tokenizer, examples, label_list, eval_batch_size=128):\n    model.to(device)\n    eval_examples = examples\n    eval_features = convert_examples_to_features(\n        eval_examples, label_list, max_seq_length, tokenizer)\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_examples))\n    logger.info(\"  Batch size = %d\", eval_batch_size)\n    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n    # Run prediction for full data\n    eval_sampler = SequentialSampler(eval_data)\n    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n\n    model.eval()\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    \n    res = []\n    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n        input_ids = input_ids.to(device)\n        input_mask = input_mask.to(device)\n        segment_ids = segment_ids.to(device)\n#         label_ids = label_ids.to(device)\n\n        with torch.no_grad():\n#             tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n            logits = model(input_ids, segment_ids, input_mask)\n\n        logits = logits.detach().cpu().numpy()\n#         print(logits)\n        res.extend(logits.argmax(-1))\n#         label_ids = label_ids.to('cpu').numpy()\n#         tmp_eval_accuracy = accuracy(logits, label_ids)\n\n#         eval_loss += tmp_eval_loss.mean().item()\n#         eval_accuracy += tmp_eval_accuracy\n\n#         nb_eval_examples += input_ids.size(0)\n        nb_eval_steps += 1\n\n#     eval_loss = eval_loss / nb_eval_steps\n#     eval_accuracy = eval_accuracy / nb_eval_examples\n#     loss = tr_loss/nb_tr_steps \n#     result = {'eval_loss': eval_loss,\n#               'eval_accuracy': eval_accuracy,\n#               'global_step': global_step,\n#               'loss': loss}\n\n#     output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n#     with open(output_eval_file, \"w\") as writer:\n#         logger.info(\"***** Eval results *****\")\n#         for key in sorted(result.keys()):\n#             logger.info(\"  %s = %s\", key, str(result[key]))\n#             writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n    return res\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c4f6c90728eb3365f31b0c3c9e1b62ed3ed7ae2c"
      },
      "cell_type": "code",
      "source": "res = predict(model, tokenizer, test_examples, label_list)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22f59bf6d9438c610df4ac02ca4702f43adc1cc2"
      },
      "cell_type": "code",
      "source": "set(res)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c693912e57b00c57a49e939e6886ab8f15cf0eb"
      },
      "cell_type": "code",
      "source": "predict(model, tokenizer, test_examples[:10], label_list)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8fdf867e0683ea48db83f2b9fdc07b84d0a1853"
      },
      "cell_type": "code",
      "source": "cat_map = {idx:lab for idx, lab in enumerate(label_list)}\nres = [cat_map[c] for c  in res]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ad089bdcced85638ff95b3a55f473c30da88d11"
      },
      "cell_type": "code",
      "source": "cat_map",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7833097cfa0f23d84d7ea9143ca257236b27017c"
      },
      "cell_type": "code",
      "source": "res",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5facd0ec0756d6975802032b51568aa15e869b0f"
      },
      "cell_type": "code",
      "source": "print(len(res))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73c0661b4a2815dcaf98ac7672b7bec6ce4a3436"
      },
      "cell_type": "code",
      "source": "test.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbc2871c2f8ff932d0ab0e487379e7687aed1261"
      },
      "cell_type": "code",
      "source": "submission = pd.read_csv('../input/ndsc-beginner/data_info_val_sample_submission.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79c5373ad607f8ee95a76043ccaafe0512fa7d80"
      },
      "cell_type": "code",
      "source": "submission.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09f570f7158c5e4c7dde05d9051dad17924c5f7e"
      },
      "cell_type": "code",
      "source": "submission['Category'] = res\nsubmission.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8ea1b4896faf34f2522aa2bb0ff32c74c5871581"
      },
      "cell_type": "code",
      "source": "submission['Category_1'],submission['Category_2'] = submission['Category'].str.split('_', 1).str",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee0188e002213e9ce54385151590d9d4e44b04e6"
      },
      "cell_type": "code",
      "source": "del submission['Category']\ndel submission['Category_1']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec15d217860e8a34b05798475be12756cee54e13"
      },
      "cell_type": "code",
      "source": "submission.columns = ['itemid','Category']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9dca4f4b2c83951dede44eb7bfccd72c6a076084"
      },
      "cell_type": "code",
      "source": "submission.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "581cdab518036d01d1b35ae1bf5973daa1eff29c"
      },
      "cell_type": "code",
      "source": "submission.to_csv('submission.csv',index= False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79b30e9d8e2614296f2d64a34c124e9ada5e410f"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}