{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "144fe5891359c058bba89c6bdef4054d3f324f34"
   },
   "outputs": [],
   "source": [
    "## Settings:\n",
    "# some config values \n",
    "max_features = 75825 #90000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 50 # max number of words in a question to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np # linear algebra\n",
    "import random\n",
    "import os \n",
    "os.environ['PYTHONHASHSEED'] = '11'\n",
    "np.random.seed(22)\n",
    "random.seed(33)\n",
    "tf.set_random_seed(44)\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, CuDNNGRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import clone_model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "bc1f67d63083cc7cbe426a288a808e9d886fd97a"
   },
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (666615, 4)\n",
      "Test shape :  (172402, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/ndsc-beginner/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/ndsc-beginner/test.csv\")\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "ba5a1b8109dee2c9fbc628d5da4a7c3447d42fb8"
   },
   "outputs": [],
   "source": [
    "## split to train and val\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n",
    "\n",
    "## fill up the missing values\n",
    "train_X = train_df[\"title\"].fillna(\"_na_\").values\n",
    "# val_X = val_df[\"title\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"title\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features,\n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'’“”')\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "# val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences \n",
    "trunc = 'pre'\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen, truncating=trunc)\n",
    "# val_X = pad_sequences(val_X, maxlen=maxlen, truncating=trunc)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen, truncating=trunc)\n",
    "\n",
    "## Get the target values\n",
    "train_y = train_df['Category'].values\n",
    "# val_y = val_df['Category'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "717f7dcd5ccf71e83d0f062e221c46db39845e4d"
   },
   "source": [
    "**Glove Embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "23f130e80159bb1701e449e2e91199dbfff1f1d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_FILE = '../input/popular-embedding/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix_1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n",
    "\n",
    "del embeddings_index; gc.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc6bab22dd12a09378f4b8b159cb7a5d88a3e7c0"
   },
   "source": [
    "**Wiki News FastText Embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "6f3d0fd28dd2b04eaccb732b96b872e5a223d962"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_FILE = '../input/popular-embedding/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix_2 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix_2[i] = embedding_vector\n",
    "        \n",
    "del embeddings_index; gc.collect()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ca44ac68bf404b9c26e07fbcc9c8ac793e04510"
   },
   "source": [
    "**Paragram Embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "25ec1aac4aedbf431a2d30de64030ce8e3203c18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_FILE = '../input/popular-embedding/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix_3 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix_3[i] = embedding_vector\n",
    "\n",
    "del embeddings_index; gc.collect()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4474db5a07969ca0703c65c3f95c2b0f61c1468b"
   },
   "source": [
    "**Word2vec Embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "9b83944a5ad66f4b5d60d57bed12b9c19035a19a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "EMBEDDING_FILE = '../input/popular-embedding/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix_4 = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    if word in embeddings_index:\n",
    "        embedding_vector = embeddings_index.get_vector(word)\n",
    "        embedding_matrix_4[i] = embedding_vector\n",
    "        \n",
    "del embeddings_index; gc.collect()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94e1be128208411c4d95a26610a4d58e66be013c"
   },
   "source": [
    "** Combine :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "957c8187e76769e3165e900a9784fc4bb30ffca4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75825, 1200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.concatenate((embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4), axis=1)  \n",
    "del embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4\n",
    "gc.collect()\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "250734f80a9799ebbe3c99dbbc16c65f599ad4ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing 431.2s\n"
     ]
    }
   ],
   "source": [
    "print(f'Done preprocessing {time.time() - t0:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "dfd358ac39ca62c6037973ad591af60362628262"
   },
   "outputs": [],
   "source": [
    "class ExponentialMovingAverage(Callback):\n",
    "    def __init__(self, model, decay=0.999, mode='epoch', n=100):\n",
    "        \"\"\"\n",
    "        mode: 'epoch': Do update_weights every epoch.\n",
    "              'batch':                   every n batches.\n",
    "        n   :\n",
    "        \"\"\"\n",
    "        self.decay = decay\n",
    "        self.mode = mode\n",
    "        self.ema_model = clone_model(model)\n",
    "        self.ema_model.set_weights(model.get_weights())\n",
    "        self.n = n\n",
    "        if self.mode is 'batch':\n",
    "            self.cnt = 0\n",
    "        self.ema_weights = [K.get_value(w) for w in model.trainable_weights]\n",
    "        self.n_weights = len(self.ema_weights)\n",
    "        super(ExponentialMovingAverage, self).__init__()\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if self.mode is 'batch':\n",
    "            self.cnt += 1\n",
    "            if self.cnt % self.n == 0:\n",
    "                self.update_weights()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if self.mode is 'epoch':\n",
    "            self.update_weights()\n",
    "        for var, w in zip(self.ema_model.trainable_weights, self.ema_weights):\n",
    "            K.set_value(var, w)\n",
    "\n",
    "    def update_weights(self):\n",
    "        for w_old, var_new in zip(self.ema_weights, self.model.trainable_weights):\n",
    "            w_old += (1 - self.decay) * (K.get_value(var_new) - w_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77c68caa1c640b9e463a196cbca50304ae134d75"
   },
   "source": [
    "**GRU:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "189e46038828bb0facf8a6e7c85582710db36dce"
   },
   "outputs": [],
   "source": [
    "def create_rnn_model(rnn, maxlen, embedding, max_features, embed_size,\n",
    "                     rnn_dim=64, dense1_dim=100, dense2_dim=50,\n",
    "                     embed_trainable=False, seed=123):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding],\n",
    "                  trainable=embed_trainable)(inp)\n",
    "    x = Dense(dense1_dim, activation='relu',\n",
    "              kernel_initializer=glorot_uniform(seed=seed))(x)\n",
    "    x = Bidirectional(rnn(rnn_dim, return_sequences=True,\n",
    "                          kernel_initializer=glorot_uniform(seed=seed)))(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(dense2_dim, activation='relu',\n",
    "              kernel_initializer=glorot_uniform(seed=seed))(x)\n",
    "    x = Dense(58, activation='softmax',\n",
    "              kernel_initializer=glorot_uniform(seed=seed))(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "9f71688f05a00613d2df4a9f009d2b56e02d22a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "666615/666615 [==============================] - 25s 38us/step - loss: 1.0920 - acc: 0.6694\n",
      "Epoch 2/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.9111 - acc: 0.7111\n",
      "Epoch 3/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.8653 - acc: 0.7227\n",
      "Epoch 4/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.8326 - acc: 0.7319\n",
      "Epoch 5/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.8047 - acc: 0.7394\n",
      "Epoch 6/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.7814 - acc: 0.7461\n",
      "Epoch 7/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.7610 - acc: 0.7518\n",
      "  n_model:1 epoch:7 Time:240.7s  25.7s/epoch\n",
      "Epoch 1/7\n",
      "666615/666615 [==============================] - 25s 37us/step - loss: 1.0661 - acc: 0.6770\n",
      "Epoch 2/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.8998 - acc: 0.7140\n",
      "Epoch 3/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.8545 - acc: 0.7258\n",
      "Epoch 4/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.8222 - acc: 0.7348\n",
      "Epoch 5/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.7950 - acc: 0.7422\n",
      "Epoch 6/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.7725 - acc: 0.7488\n",
      "Epoch 7/7\n",
      "666615/666615 [==============================] - 24s 36us/step - loss: 0.7525 - acc: 0.7543\n",
      "  n_model:2 epoch:7 Time:235.5s  24.7s/epoch\n",
      "Epoch 1/7\n",
      "536576/666615 [=======================>......] - ETA: 4s - loss: 1.1131 - acc: 0.6657"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_ids = [list(range(300)), list(range(300, 600)),\n",
    "             list(range(600, 900)), list(range(900, 1200))]\n",
    "embed_ids_dict = {1: [embed_ids[0], embed_ids[1], embed_ids[2], embed_ids[3]],\n",
    "                  2: [embed_ids[0] + embed_ids[1],\n",
    "                      embed_ids[0] + embed_ids[2],\n",
    "                      embed_ids[0] + embed_ids[3],\n",
    "                      embed_ids[1] + embed_ids[2],\n",
    "                      embed_ids[1] + embed_ids[3],\n",
    "                      embed_ids[2] + embed_ids[3]],\n",
    "                  3: [embed_ids[0] + embed_ids[1] + embed_ids[2],\n",
    "                      embed_ids[0] + embed_ids[1] + embed_ids[3],\n",
    "                      embed_ids[0] + embed_ids[2] + embed_ids[3],\n",
    "                      embed_ids[1] + embed_ids[2] + embed_ids[3]],\n",
    "                  4: [embed_ids[0] + embed_ids[1] + embed_ids[2] + embed_ids[3]]}\n",
    "embed_ids_lst = embed_ids_dict[2]\n",
    "embed_size = 600\n",
    "\n",
    "rnn = CuDNNGRU\n",
    "embed_trainable = False\n",
    "\n",
    "n_models = 6\n",
    "epochs = 7\n",
    "batch_size = 512\n",
    "dense1_dim = rnn_dim = 128\n",
    "dense2_dim = 2 * rnn_dim\n",
    "\n",
    "ema_n = int(len(train_y) / batch_size / 10)\n",
    "decay = 0.9\n",
    "scores = []\n",
    "\n",
    "oof_pred = np.zeros((len(train_X),58))\n",
    "# pred_avg = np.zeros((len(val_y), 58))\n",
    "pred_test_avg = np.zeros((test_df.shape[0], 58))\n",
    "for i in range(n_models):\n",
    "    t1 = time.time()\n",
    "    seed = 101 + 11 * i\n",
    "    cols_in_use = embed_ids_lst[i % len(embed_ids_lst)]\n",
    "    model = create_rnn_model(rnn, maxlen, embedding_matrix[:, cols_in_use],\n",
    "                             max_features, embed_size,\n",
    "                             rnn_dim=rnn_dim,\n",
    "                             dense1_dim=dense1_dim,\n",
    "                             dense2_dim=dense2_dim,\n",
    "                             embed_trainable=embed_trainable,\n",
    "                             seed=seed)\n",
    "    ema = ExponentialMovingAverage(model, decay=decay, mode='batch', n=ema_n)\n",
    "    model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs,\n",
    "              callbacks=[ema], verbose=1)\n",
    "    m = ema.ema_model\n",
    "    t_per_epoch = (time.time() - t1) / epochs\n",
    "#     pred = m.predict([val_X])\n",
    "    train_pred = m.predict([train_X])\n",
    "#     print(pred.shape)\n",
    "    oof_pred += train_pred\n",
    "    pred_test = m.predict([test_X])\n",
    "    pred_test_avg += pred_test\n",
    "#     f1_one, thresh_one = f1_best(val_y, pred)\n",
    "#     f1_avg, thresh_avg = f1_best(val_y, pred_avg / (i + 1))\n",
    "#     nll_one = metrics.log_loss(val_y, pred)\n",
    "#     nll_avg = metrics.log_loss(val_y, pred_avg / (i + 1))\n",
    "#     auc_one = metrics.roc_auc_score(val_y, pred)\n",
    "#     auc_avg = metrics.roc_auc_score(val_y, pred_avg)\n",
    "    print(f'  n_model:{i + 1} epoch:{epochs} ' +\n",
    "          f'Time:{time.time() - t1:.1f}s  {t_per_epoch:.1f}s/epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "151ee97c9f1aa3039c68c96c95efcadad9211437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish saving numpy array\n",
      "finished saving oof file\n",
      "Done:1874.1s\n"
     ]
    }
   ],
   "source": [
    "pred_test_avg /= n_models\n",
    "np.save('pred_EMA.np',pred_test_avg )\n",
    "print('finish saving numpy array')\n",
    "\n",
    "oof_pred /= n_models\n",
    "np.save('oof_EMA.np',oof_pred)\n",
    "print('finished saving oof file')\n",
    "\n",
    "y_te = [np.argmax(preds) for preds in pred_test_avg]\n",
    "\n",
    "# pred_test_avg = (pred_test_avg>thresh_avg).astype(int)\n",
    "out_df = pd.DataFrame({\"title\":test_df[\"title\"].values})\n",
    "out_df['Category'] = y_te\n",
    "out_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(f'Done:{time.time() - t0:.1f}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
